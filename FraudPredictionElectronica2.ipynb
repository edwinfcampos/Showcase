{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Prediction at Electronica \n",
    "## Part 2: Deployable Feature Engineering and Logistic Regression Algorithm\n",
    "\n",
    "\n",
    "This outputs a machine learning model that predicts the probability \n",
    "that the first transaction of a new user is fraudulent.\n",
    "\n",
    "If you fail to identify a fraudulent transaction, \n",
    "Electronica loses money equivalent to the price of the fraudulently purchased product. \n",
    "\n",
    "If you incorrectly flag a real transaction as fraudulent, \n",
    "it inconveniences the Electronica customers whose valid transactions are flaggedâ€”a cost your client values at $8.\n",
    "\n",
    "    Created by Edwin Campos on 2020 Feb.18\n",
    "    Last modification on 2020 Feb.18 by ecampos.phd@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# Clean the memory\n",
    "%reset -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import fbeta_score, f1_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '/Users/ecampos/AnacondaProjects/DS-Fraud-Univar/Fraud - For Candidate/Fraud - For Candidate/'\n",
    "INPUT_FILE = 'fraud.csv'\n",
    "IP2COUNTRY_FILE = 'IpAddress_to_Country.csv'\n",
    "TARGET = 'class'   # Column in dataframe with target values, Fraud:1, Good:0\n",
    "MODEL_FILENAME = 'fraud_model_logisticRegression.sav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to convert from ip address into country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ip2country(ip_number_df,lookup_df):\n",
    "    \"\"\"Outputs the country where the numeric IP address is located.\n",
    "    Call it as...\n",
    "        convert_ip2country(df_all, df_ip2country)\n",
    "    Inputs:\n",
    "       ip_number_df -- Pandas Dataframe or Series with numeric values corresponding to an Internet Protocol address \n",
    "                       For example ips_df = pd.Series({'ip_address':[16777216,16777210.0]})\n",
    "       lookup_df -- Pandas Dataframe name with information to map an IP address to its country\n",
    "    Output:\n",
    "       output_df -- Pandas Series with the country name of each IP address\n",
    "    \"\"\"\n",
    "    #Initialize output dataframe\n",
    "    output_df = pd.DataFrame(columns=['ip_address'])\n",
    "    for ip_number in ip_number_df['ip_address']:\n",
    "        lookup_df_1 = lookup_df[ lookup_df['lower_bound_ip_address'] <= ip_number ]\n",
    "        if lookup_df_1.empty:\n",
    "            country = 'ErrorTooLowIP'\n",
    "        else:\n",
    "            lookup_df_2 = lookup_df_1[lookup_df_1['upper_bound_ip_address'] >= ip_number]\n",
    "            if lookup_df_2.empty:\n",
    "                country = 'ErrorTooHighIP'\n",
    "            else:\n",
    "                country = lookup_df_2['country'].iloc[0]\n",
    "        output_df = output_df.append({'ip_address': country }, ignore_index=True)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(input_csv, target_column, ip2country_file, impute=False):\n",
    "    \"\"\"Builds the features for the Electronica Fraud model\n",
    "    Call it as...\n",
    "        X_test, y_test = engineer_features(INPUT_PATH+INPUT_FILE, TARGET, INPUT_PATH+IP2COUNTRY_FILE)\n",
    "    Inputs: \n",
    "        input_csv -- string path to input dataset, for example, INPUT_PATH+INPUT_FILE\n",
    "        target_column -- string with column name of target variable, for example, TARGET\n",
    "        ip2country_file -- file with the CSV lookup table to match IP addresses to its countries, for example, INPUT_PATH+IP2COUNTRY_FILE\n",
    "        impute -- Optional indtruction to whether or not impute missing values, for example, True, False, or None\n",
    "    Outputs: \n",
    "        X_t with model input-feature values and y_t with model target values\n",
    "    \"\"\"\n",
    "    # Ingest Input dataset\n",
    "    df_all = pd.read_csv(input_csv, low_memory=False)\n",
    "    \n",
    "    # Impute Missing values: If Nan, then make it zero and create a flag column accordingly\n",
    "    if impute:\n",
    "        columns2impute = df_all.columns\n",
    "        for col in columns2impute:\n",
    "            df_all[col+'_flag']=np.where(df_all[col].isnull()==True,1,0)\n",
    "        df_all = df_all.fillna(0)\n",
    "    \n",
    "    # Convert from Dates and Times into Seconds since Epoch\n",
    "    df_all['signup_datetime'] = pd.to_datetime(df_all['signup_time'])\n",
    "    df_all['signup_epoch'] = (df_all['signup_datetime'] - dt.datetime(1970,1,1)).dt.total_seconds()\n",
    "    df_all['purchase_datetime'] = pd.to_datetime(df_all['purchase_time'])\n",
    "    df_all['purchase_epoch'] = (df_all['purchase_datetime']  - dt.datetime(1970,1,1)).dt.total_seconds()\n",
    "    \n",
    "    # Create new column features for days of week and period between signup and first purchase\n",
    "    # days_name = {0:'Mon',1:'Tues',2:'Weds',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}\n",
    "    df_all['signup_dayofweek'] = df_all['signup_datetime'].dt.dayofweek\n",
    "    df_all['purchase_dayofweek'] = df_all['purchase_datetime'].dt.dayofweek\n",
    "    df_all[\"purchase_signup_epoch\"] = (df_all['purchase_epoch'] - df_all['signup_epoch'])\n",
    "    \n",
    "    # Removing Non-Word Characters (% @ \" ? % $) using Regular Expressions\n",
    "    df_all['purchase_value_numeric'] = df_all['purchase_value'].replace(r\"[@\\'?\\$%_]\", \"\", regex=True).apply(pd.to_numeric, errors='raise')\n",
    "    \n",
    "    # Ingest dataset with lookup table\n",
    "    df_ip2country = pd.read_csv(ip2country_file)\n",
    "    \n",
    "    # Create additional column with ip_country\n",
    "    df_all['ip_country'] = convert_ip2country(df_all, df_ip2country )\n",
    "    \n",
    "    # Remove special characters from ip_country column\n",
    "    df_all['ip_country_clean'] = df_all['ip_country'].str.replace('\\W', '')\n",
    "    \n",
    "    # Apply One-Hot-Encoding to columns ip_country, source, browser, and sex\n",
    "    onehotencoding_columns = ['ip_country_clean','source', 'browser', 'sex']  # On column 'source', SEO corresponds to Search Engine Optimization\n",
    "    for col in onehotencoding_columns:\n",
    "        df_dummies = pd.get_dummies(df_all[col], prefix = col)\n",
    "        df_all = pd.concat([df_all, df_dummies], axis=1)\n",
    "        \n",
    "    # Identify features and target columns\n",
    "    unwanted_columns = ['signup_epoch',  # Redundant as per Covariance Matrix\n",
    "                        'Unnamed: 0', 'user_id', 'device_id',  # irrelevant or random at moment of using model\n",
    "                        'signup_time', 'purchase_time', 'purchase_value', #other numeric columns were created instead\n",
    "                        'ip_country','ip_country_clean', # Other One-hot-encoded columns created instead\n",
    "                        'source', 'browser', 'sex',  # Other One-hot-encoded columns created instead\n",
    "                        'signup_datetime', 'purchase_datetime'  # Other numeric columns created instead\n",
    "                       ]\n",
    "    unwanted_columns.append(target_column)\n",
    "    feature_columns =[x for x in df_all.columns if x not in unwanted_columns]\n",
    "    X_t = df_all[feature_columns]\n",
    "    y_t = df_all[TARGET]    \n",
    "    \n",
    "    return X_t, y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to calculate money lost by Electronica due to model output errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_target, y_predict, X_input):\n",
    "    \"\"\"Calculates the average dollars lost per wrong prediction, for the Electronica Fraud model\n",
    "    Call it as\n",
    "        custom_loss( X_input=X_test, y_target=y_test, y_predict=y_pred_test )\n",
    "    Inputs: \n",
    "        y_target -- True target values\n",
    "        y_predict -- Target values predicted by the model\n",
    "        X_input -- Model input-feature values\n",
    "    Outputs:\n",
    "        ratio = amount of dollars lost by model errors / number of predictions done by model\n",
    "    \"\"\"\n",
    "    error = 0\n",
    "    #for i in range(y_target.shape[0]):  # Use this for Pandas\n",
    "    for i in range(len(y_target)):\n",
    "        #False Negatives\n",
    "        #if (y_predict.iloc[i].item() == 0) and (y_target.iloc[i].item() == 1):  # Use this for Pandas\n",
    "        if (y_predict[i] == 0) and (y_target[i] == 1):                           # Use this for Numpy\n",
    "            error_i = X_input.purchase_value_numeric.iloc[i]          \n",
    "        #False Positives\n",
    "        #elif y_predict.iloc[i].item() == 1 and y_target.iloc[i].item() == 0:    # Use this for Pandas\n",
    "        elif y_predict[i] == 1 and y_target[i] == 0:     # Use this for Numpy\n",
    "            error_i = 8\n",
    "        else:\n",
    "            error_i = 0\n",
    "        error += error_i\n",
    "    return error/len(y_target)\n",
    "\n",
    "custom_scorer = make_scorer(custom_loss, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This Procedure generates a matplotlib object to be used in a confusion matrix image.\n",
    "    Use it as...\n",
    "        plot_confusion_matrix(conf_matrix_test, \n",
    "                              classes=['0:Good','1:Fraud'], \n",
    "                              title=\"Test Confusion Matrix, Counts\")\n",
    "        plt.figure()\n",
    "        plt.show\n",
    "    Inputs:\n",
    "        cm -- confusion matrix values\n",
    "        classes -- Strings to be used as labels next to plot axes tickmarks, corresponding to each target class\n",
    "        title -- Strings with the plot title\n",
    "    output:\n",
    "        plt -- A matplotlib object\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j,i,int(round(cm[i,j])),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take ~17 min running\n",
    "X_test, y_test = engineer_features(INPUT_PATH+INPUT_FILE, TARGET, INPUT_PATH+IP2COUNTRY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The code below is to run the code with older model, using only 21 features in X_test2\n",
    "\n",
    "    older_model_columns = [x for x in X_test.columns if 'ip_country_clean' not in x]\n",
    "    print(older_model_columns)\n",
    "    print(len(older_model_columns))\n",
    "    print(\"Missing: 'signup_epoch',  'Unnamed: 0', 'ip_country' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    print(X_test.shape)\n",
    "    df_all = pd.read_csv(INPUT_PATH+INPUT_FILE, low_memory=False)\n",
    "    df_all = df_all[[ 'user_id', 'Unnamed: 0', 'ip_address' ]]\n",
    "    print(df_all.shape)\n",
    "    X_test2 = X_test.drop(older_model_columns, axis=1)\n",
    "    X_test2 = pd.concat([X_test2, df_all], axis=1)\n",
    "    print(X_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(MODEL_FILENAME, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "\n",
    "print(result)\n",
    "\n",
    "print(classification_report(y_test, \n",
    "                            loaded_model.predict(X_test)\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test.shape, loaded_model.coef_.shape)\n",
    "print(X_test.shape, loaded_model.best_estimator_.coef_.shape)\n",
    "\n",
    "print(TARGET+' =')\n",
    "for i in range(X_test.shape[1]):\n",
    "    print( loaded_model.best_estimator_.coef_[0][i],'*',X_test.columns[i] )\n",
    "    if i != X_test.shape[1]:\n",
    "        print('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix in counts\n",
    "plt.figure() \n",
    "plot_confusion_matrix(conf_matrix_test, classes=['0:Good','1:Fraud'], \n",
    "                      title=\"Test Confusion Matrix, Counts\")\n",
    "plt.figure()\n",
    "#plt.show\n",
    "plt.savefig('conf_matrix_counts_test.png')\n",
    "\n",
    "# Plot Confusion Matrix in percents\n",
    "conf_matrix_test_percents = 100.0 * conf_matrix_test /float(len(y_test))\n",
    "plot_confusion_matrix(conf_matrix_test_percents, classes=['0:Good','1:Fraud'],\n",
    "                      title=\"Test Confusion Matrix, Percents\")\n",
    "plt.show\n",
    "plt.savefig('conf_matrix_prcnts_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict values based on new parameters\n",
    "y_pred_test = loaded_model.predict(X_test)\n",
    "\n",
    "# New Model Evaluation metrics\n",
    "print('Custom Lost Function: $' + str(custom_loss(X_input=X_test, \n",
    "                                                  y_target=y_test, \n",
    "                                                  y_predict=y_pred_test)) )\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_test)))\n",
    "print('Precision Score : ' + str(precision_score(y_test,y_pred_test)))\n",
    "print('Recall Score : ' + str(recall_score(y_test,y_pred_test)))\n",
    "print('F1 Score : ' + str(f1_score(y_test,y_pred_test)))\n",
    "\n",
    "# Confusion Matrix on test dataset\n",
    "conf_matrix_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work -> This part is under construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POR ACA VOY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cross Validation using the area under the Receiver Operating Characteristic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = model_logistic_reg\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(MODEL, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is using Class Weighted XGBoost, or Cost-Sensitive XGBoost, which can offer better performance on binary classification problems with a severe class imbalance.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/xgboost-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = XGBClassifier(objective='binary:logistic', n_estimators=10, seed=21)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "predic = xg_cl.predict(X_test)\n",
    "probab = xg_cl.predict_proba(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(predic==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model: eXtreame Gradient Boosting\n",
    "model = XGBClassifier()\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, # Number of folds\n",
    "                             n_repeats=3, # Number of times cross-validator will be repeated\n",
    "                             random_state=1)  # For reproducible results\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.5f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using stratified cross validation to evaluate an XGBoost model.\n",
    "\n",
    "If you have many classes for a classification type predictive modeling problem or the classes are imbalanced (there are a lot more instances for one class than another), it can be a good idea to create stratified folds when performing cross validation.\n",
    "\n",
    "This has the effect of enforcing the same distribution of classes in each fold as in the whole training dataset when performing the cross validation evaluation. The scikit-learn library provides this capability in the StratifiedKFold class.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation method: stratified k-fold cross validation evaluation of xgboost model\n",
    "#from numpy import loadtxt\n",
    "#import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validated model\n",
    "model = XGBClassifier()\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
